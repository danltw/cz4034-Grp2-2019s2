{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    poll south carolina minute closing voter cast ...\n",
      "1    united state top doctor one simple request sto...\n",
      "2    around south carolina voter democratic primary...\n",
      "3    alexa negr n luciano transgender woman kill ci...\n",
      "4    demographic south carolina democratic primary ...\n",
      "Name: clean_tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "\n",
    "words = set(nltk.corpus.words.words())\n",
    "stop_list = []\n",
    "\n",
    "# create a list of stop_words\n",
    "def create_custom_stopwords():\n",
    "    stop_list = set(stopwords.words('english'))\n",
    "    return stop_list\n",
    "\n",
    "# remove punctuations from tweet\n",
    "def remove_punctuation(data):\n",
    "    result = \"\".join(re.sub(\"([\\[\\]\\&\\$\\%\\(\\):\\/]|(\\.,)|(\\.\\.)\\.*|(\\-)|(\\!\\!)+|[^\\x00-\\x7F])+\", \" \", t) for t in data)\n",
    "    result = \"\".join(w for w in result if w not in string.punctuation)\n",
    "    return result\n",
    "\n",
    "# remove numbers from tweet\n",
    "def remove_numbers(data):\n",
    "    result = \"\".join(re.sub(\"(\\d)+\", \" \", t) for t in data)\n",
    "    return result\n",
    "\n",
    "# remove stopwords from tweet\n",
    "def remove_stopwords(data):\n",
    "    result = \" \".join(t for t in data.split() if t.lower() not in stop_list)\n",
    "    return result\n",
    "\n",
    "# apply stemming on tweet to get root words\n",
    "def stemming(data):\n",
    "    stemmer = PorterStemmer()\n",
    "    result = \" \".join(stemmer.stem(t) for t in data.split())\n",
    "    return result\n",
    "\n",
    "# apply lemmatization on tweet to get root words\n",
    "def lemmatize(tweet):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    tagged_tokens = nltk.pos_tag(word_tokens)\n",
    "    result = \"\"\n",
    "    for word, tag in tagged_tokens:\n",
    "        pos_tag = get_wordnet_pos(tag)\n",
    "        if pos_tag is None:\n",
    "            result += \" \" + lemmatizer.lemmatize(word)\n",
    "        else:\n",
    "            result += \" \" + lemmatizer.lemmatize(word, pos=pos_tag)\n",
    "    return result\n",
    "\n",
    "# implement part-of-speech tagging to prepare for lemmatization\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# split tweet into word tokens\n",
    "def tokenize(tweets):\n",
    "    result = []\n",
    "    for tweet in tweets:\n",
    "        word_tokens = word_tokenize(tweet)\n",
    "        result.append(word_tokens)\n",
    "    return result\n",
    "\n",
    "# collection of methods to clean tweet content\n",
    "def normalize(tweet):\n",
    "    tweet = tweet.apply(lambda x: x.lower())\n",
    "    tweet = tweet.apply(lambda x: remove_numbers(x))\n",
    "    tweet = tweet.apply(lambda x: remove_punctuation(x))\n",
    "    return tweet\n",
    "\n",
    "# convert dataset to dataframe\n",
    "tweet_df = pd.read_csv('finalOutput_crawled.csv', encoding = \"ISO-8859-1\", header = 0)\n",
    "\n",
    "# initializing list of custom stopwords\n",
    "stop_list = create_custom_stopwords()\n",
    "\n",
    "tweet_df = tweet_df.iloc[:,0:3]\n",
    "\n",
    "# adding an extra column to store the cleaned tweets\n",
    "tweet_df[\"clean_tweet\"] = tweet_df.iloc[:,1]\n",
    "\n",
    "# remove http/https links, RT\n",
    "tweet_df[\"clean_tweet\"] = tweet_df[\"clean_tweet\"].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "\n",
    "tweet_df[\"clean_tweet\"] = tweet_df[\"clean_tweet\"].apply(lambda x: re.split('RT', str(x))[0])\n",
    "\n",
    "tweet_df[\"clean_tweet\"] = tweet_df[\"clean_tweet\"].apply(lambda x: re.split('http|https', str(x))[0])\n",
    "\n",
    "# clean the tweet contents\n",
    "tweet_df[\"clean_tweet\"] = normalize(tweet_df[\"clean_tweet\"])\n",
    "\n",
    "# convert to root form\n",
    "tweet_df[\"clean_tweet\"] = tweet_df[\"clean_tweet\"].apply(lambda x: lemmatize(x))\n",
    "\n",
    "# remove stopwords\n",
    "tweet_df[\"clean_tweet\"] = tweet_df[\"clean_tweet\"].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "# storing new dataset into file\n",
    "tweet_df.to_csv(\"finalOutput_cleaned.csv\",index=False)\n",
    "\n",
    "print(tweet_df[\"clean_tweet\"].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
